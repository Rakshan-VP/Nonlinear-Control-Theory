\chapterimage{orange2.jpg} % Chapter heading image
\chapterspaceabove{6.75cm} % Whitespace from the top of the page to the chapter title on chapter pages
\chapterspacebelow{7.25cm} % Amount of vertical whitespace from the top margin to the start of the text on chapter pages

\chapter{Input-Output Stability}\index{Input-Output Stability}

\section{Overview}\index{Overview}

So far, stability was studied using Lyapunov functions focusing on internal states. \textbf{Input-Output (I/O) Stability}, in contrast, studies how bounded inputs produce bounded outputs using the mapping $y = H(u)$. It treats the system as a black box without considering internal states.  

\begin{center}
\begin{tikzpicture}[auto, node distance=1.5cm, thick]
  \node (u) {$u(t)$};
  \node[draw, rectangle, minimum height=1.5cm, minimum width=2.5cm, right=of u] (H) {$H$};
  \node (y) [right=of H] {$y(t)$};
  \draw[->] (u) -- (H);
  \draw[->] (H) -- (y);
\end{tikzpicture}
\end{center}

\section{Function Spaces}\index{Function Spaces}

So far we studied vector spaces of finite-dimensional vectors.  
Now we study \textbf{function spaces}, where the elements are \emph{functions} instead of vectors.  

\begin{definition}[Function space]
A \emph{function space} is a set of functions from a domain (e.g.\ time, $\mathbb{R}_{\geq 0}$) into a codomain (e.g.\ $\mathbb{R}^q$) that is equipped with additional structure, such as operations (addition, scalar multiplication) and possibly a norm or inner product.  
For example, we may consider
\begin{equation}
u:\mathbb{R}_{\geq 0} \to \mathbb{R}^q, 
\quad 
u(t) =
\begin{bmatrix}
u_1(t) \\ \vdots \\ u_q(t)
\end{bmatrix}.
\end{equation}
\end{definition}

\subsection{$L_p$ Spaces}

A particularly important class of function spaces in analysis and control are the $L_p$ spaces.

\begin{definition}[$L_p$ space]
For $1 \leq p < \infty$, the space $L_p(0,\infty;\mathbb{R}^q)$ is defined as
\begin{equation}
L_p = \Big\{ u:\mathbb{R}_{\geq 0}\to \mathbb{R}^q \;\Big|\;
\int_{0}^{\infty} \|u(t)\|^p \, dt < \infty \Big\}.
\end{equation}
The corresponding norm is
\begin{equation}
\|u\|_{L_p} = \left( \int_{0}^{\infty} \|u(t)\|^p \, dt \right)^{1/p}.
\end{equation}
\end{definition}

\begin{definition}[$L_2$ space]
The \emph{square-integrable space} is the case $p=2$:
\begin{equation}
L_2(0,\infty;\mathbb{R}^q) 
= \Big\{ u:\mathbb{R}_{\geq 0}\to \mathbb{R}^q \;\Big|\;
\int_{0}^{\infty} \|u(t)\|^2 \, dt < \infty \Big\}.
\end{equation}
The norm
\begin{equation}
\|u\|_{L_2} = \left( \int_{0}^{\infty} \|u(t)\|^2 \, dt \right)^{1/2}
\end{equation}
represents the \textbf{energy} of the signal.
\end{definition}

\begin{definition}[$L_\infty$ space]
The \emph{essentially bounded space} is defined as
\begin{equation}
L_\infty(0,\infty;\mathbb{R}^q) 
= \Big\{ u:\mathbb{R}_{\geq 0}\to \mathbb{R}^q \;\Big|\;
\sup_{t \geq 0} \|u(t)\| < \infty \Big\}.
\end{equation}
The norm
\begin{equation}
\|u\|_{L_\infty} = \sup_{t \geq 0} \|u(t)\|
\end{equation}
represents the \textbf{maximum amplitude} of the signal.
\end{definition}

\begin{remark}
\begin{itemize}
\item  $L_2$ is widely used to measure signal energy (that's why used in stability analysis).  
\item  $L_\infty$ is used to measure boundedness or peak values of signals.  
\item  In general, $L_p$ spaces allow us to study signals with different growth and smoothness properties, and they provide the foundation for many results in control and signal processing.
\end{itemize}
\end{remark}

\begin{proposition}[Hölder’s inequality]
Let $p,q > 1$ with $\tfrac{1}{p}+\tfrac{1}{q}=1$.  
If $f \in L_p(0,T)$ and $g \in L_q(0,T)$, then $fg \in L_1(0,T)$ and 
\begin{equation}
\int_{0}^{T} |f(t)g(t)| \, dt 
\;\leq\;
\left( \int_{0}^{T} |f(t)|^p \, dt \right)^{1/p}
\left( \int_{0}^{T} |g(t)|^q \, dt \right)^{1/q}.
\end{equation}
\end{proposition}

\begin{proposition}[Minkowski’s inequality]
For $1 \leq p < \infty$, if $f,g \in L_p(0,T)$, then
\begin{equation}
\|f+g\|_{L_p} \;\leq\; \|f\|_{L_p} + \|g\|_{L_p}.
\end{equation}
Thus $\|\cdot\|_{L_p}$ defines a norm on $L_p$.
\end{proposition}

\subsection{Extended Spaces}\index{Function Spaces!Extended}

\begin{definition}[Truncation operator]
Let $X$ be a function space on $[0,\infty)$.  
For $T>0$, the \emph{truncation operator} $P_T:X \to X$ is defined by
\begin{equation}
(P_Tu)(t) =
\begin{cases}
u(t), & 0 \leq t \leq T,\\[6pt]
0, & t > T.
\end{cases}
\end{equation}
\end{definition}

\begin{definition}[Extended space]
Let $X$ be a function space on $[0,\infty)$.  
The \emph{extended space} $X_e$ is defined as
\begin{equation}
X_e = \big\{ u:[0,\infty)\to\mathbb{R}^q \;\big|\; 
P_T u \in X \;\; \text{for all } T>0 \big\}.
\end{equation}
In words: $u \in X_e$ if \textbf{every finite-time truncation} of $u$ belongs to $X$.
\end{definition}

\begin{example}
If $X = L_2(0,\infty)$, then
\begin{equation}
X_e = \{ u:[0,\infty)\to \mathbb{R}^q \mid u \in L_2(0,T) \;\;\forall T>0\}.
\end{equation}
Thus $X_e$ contains all signals that have \textbf{finite energy on every bounded interval}, even if their total energy on $[0,\infty)$ is infinite.  
For example, $u(t)=1$ is not in $L_2(0,\infty)$, but $u \in (L_2)_e$ since its truncation to any finite interval lies in $L_2(0,T)$.
\end{example}

\begin{remark}
The extended space $X_e$ enlarges $X$ by admitting signals that are locally in $X$ but not globally.  
This is useful in control theory because real signals are often studied over finite horizons.
\end{remark}

\section{Input-Output Stability}\index{Input-Output Stability}

\begin{definition}[Mapping]
The mathematical representation of a physical system is defined as a mapping
\begin{equation}
H: X_e \to X_e
\end{equation}
that satisfies the \textbf{causality condition}:
\begin{equation}
(Hu)_T = \big(H(u_T)\big)_T, \qquad \forall u \in X_e, \; T>0,
\end{equation}
where $X_e$ is the space consisting of all functions whose truncation belongs to $X$.
\end{definition}

\begin{example}
Consider the first-order causal system:
\begin{equation}
\dot{y}(t) + y(t) = u(t), \qquad y(0) = 0.
\end{equation}

Let the input $u(t)$ be a step function. Its truncated version at $T=4$ is denoted $u_T(t)$. Then the system outputs satisfy the causality condition:
\begin{equation}
(Hu)_T = (H(u_T))_T.
\end{equation}

\begin{center}
\begin{tikzpicture}[auto, node distance=1.5cm, thick]
  % First: full input
  \node (u) {$u(t)$};
  \node[draw, rectangle, minimum height=1.5cm, minimum width=2.5cm, right=of u] (H) {$H$};
  \node (y) [right=of H] {$(Hu)(t)$};
  \draw[->] (u) -- (H);
  \draw[->] (H) -- (y);
\end{tikzpicture}

\vspace{1cm}

\begin{tikzpicture}[auto, node distance=1.5cm, thick]
  % Second: truncated input
  \node (uT) {$u_T(t)$};
  \node[draw, rectangle, minimum height=1.5cm, minimum width=2.5cm, right=of uT] (H) {$H$};
  \node (yT) [right=of H] {$(H(u_T))(t)$};
  \draw[->] (uT) -- (H);
  \draw[->] (H) -- (yT);
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tikzpicture}
\begin{axis}[
    width=12cm, height=5cm,
    xlabel={$t$}, ylabel={$u(t)$},
    xmin=0, xmax=8, ymin=0, ymax=1.2,
    samples=200, domain=0:8,
    legend style={at={(0.98,0.5)},anchor=east}
]
\addplot[blue, thick] {x>=0 ? 1 : 0}; 
\addlegendentry{$u(t)$ (step)}

\addplot[red, dashed, thick] {x<=4 ? 1 : 0}; 
\addlegendentry{$u_T(t)$, $T=4$}
\end{axis}
\end{tikzpicture}

\vspace{0.5cm}

\begin{tikzpicture}
\begin{axis}[
    width=12cm, height=5cm,
    xlabel={$t$}, ylabel={$y(t)$},
    xmin=0, xmax=8, ymin=0, ymax=1.2,
    legend style={at={(0.98,0.5)},anchor=east}
]
% Full system response
\addplot[blue, thick, samples=200, domain=0:8] {1 - exp(-x)};
\addlegendentry{$(Hu)(t)$}

% Truncated input system response
\addplot[red, dashed, thick, samples=200, domain=0:8] {x<=4 ? (1 - exp(-x)) : (1 - exp(-4))*exp(-(x-4))};
\addlegendentry{$(H(u_T))(t)$}

% Output truncation
\addplot[green!70!black, dotted, thick, samples=200, domain=0:8] {x<=4 ? (1 - exp(-x)) : 0};
\addlegendentry{$(Hu)_T$}
\end{axis}
\end{tikzpicture}
\end{center}
\end{example}

\begin{remark}
For a causal system, the output at any time $t$ depends only on the input values up to that time. Therefore, if we truncate the input at time $T$ to obtain $u_T(t)$, the resulting output $(H(u_T))(t)$ matches the original output $(Hu)(t)$ for all $t \le T$. Moreover, truncating the original output at $T$, i.e., $(Hu)_T$, gives exactly the same signal. This property formally captures the essence of input-output causality.
\end{remark}

\begin{definition}[Finite-Gain System]
A system $H$ is said to have a \textbf{finite gain} if there exist constants 
\(\gamma(H) < \infty\) (called the \emph{gain} of $H$) and \(\beta \in \mathbb{R}^+\) such that, for all inputs \(u \in X_e\),
\begin{equation}
\| Hu \|_X \le \gamma(H) \, \| u \|_X + \beta.
\end{equation}

Here, \(\beta\) is a bias term which may be nonzero even when \(u = 0\).  
If \((Hu) = 0\) whenever \(u = 0\), then \(\beta = 0\), and the gain can be calculated as
\begin{equation}
\gamma(H) = \sup_{u \neq 0} \frac{\| (Hu)_T \|_X}{\| u_T \|_X}.
\end{equation}
\end{definition}

\begin{example}[First-Order Linear System]
Consider the system
\begin{equation}
\dot{y}(t) + y(t) = u(t), \quad y(0)=0,
\end{equation}
with input \(u(t) \in L_\infty[0,\infty)\).  

- The output is 
\begin{equation}
y(t) = \int_0^t e^{-(t-\tau)} u(\tau) \, d\tau.
\end{equation}  

- Using the norm \(\| \cdot \|_\infty\), we can estimate
\begin{equation}
|y(t)| \le \int_0^t e^{-(t-\tau)} |u(\tau)| \, d\tau \le \| u \|_\infty \int_0^t e^{-(t-\tau)} d\tau \le \| u \|_\infty.
\end{equation}

Thus, the system has \textbf{finite gain} with 
\begin{equation}
\gamma(H) = 1, \quad \beta = 0.
\end{equation}
\end{example}

\section{LTI Systems}\index{LTI Systems}

We study Linear Time-Invariant (LTI) systems in terms of input-output behavior instead of state-space models, focusing on SISO systems for simplicity.  

%------------------------------------------------
\begin{definition}[Set $A$]  
A function $f$ belongs to the set $\mathcal{A}$ if
\begin{equation}
f(t) = f_0\delta(t) + f_a(t), \quad t \geq 0,
\end{equation}
and $f(t) = 0$ for $t < 0$, where
\begin{equation}
f_a \in L_1, \quad \int_0^\infty |f_a(\tau)| \, d\tau < \infty.
\end{equation}
The norm is defined as
\begin{equation}
\|f_a\| = f_0 + \int_0^\infty f_a(t)\, dt.
\end{equation}
\end{definition}

\begin{definition}[Laplace Transform Set]  
Let $\hat{\mathcal{A}}$ be the set of Laplace transforms of $A$. That is,
\begin{equation}
\hat{\mathcal{A}} = \{ F(s) = \mathcal{L}\{f(t)\} \mid f \in A \}.
\end{equation}
\end{definition}

\begin{definition}[Proper and Strictly Proper Rational Functions]  
Let $\mathbb{R}[s]$ denote the set of polynomials in $s$, and $\mathbb{R}(s)$ the field of fractions of $\mathbb{R}[s]$.  

A rational function $\hat{M}(s) \in \mathbb{R}(s)$ is called:  
\begin{itemize}
    \item \textbf{Proper} if $\displaystyle \lim_{s \to \infty} \hat{M}(s) < \infty$; equivalently, the degree of the numerator is less than or equal to the degree of the denominator in the Laplace domain.
    \item \textbf{Strictly Proper} if $\displaystyle \lim_{s \to \infty} \hat{M}(s) = 0$; equivalently, the degree of the numerator is strictly less than that of the denominator in the Laplace domain.
\end{itemize}
According to this, $H^\cap(s)$ (Laplace transform of the impulse response) is strictly proper.
\end{definition}

\begin{theorem}  
If $\hat{F}(s) \in \mathbb{R}(s)$, then $F(s) \in \hat{\mathcal{A}}$ if and only if:
\begin{enumerate}
    \item $F^\cap(s)$ is proper, and  
    \item All poles of $F^\cap(s)$ lie in the left-half plane.  
\end{enumerate}
\end{theorem}

\begin{definition}[Convolution]  
For functions $f$ and $g$, their convolution is defined as
\begin{equation}
(f * g)(t) = \int_0^t f(\tau) g(t-\tau)\, d\tau.
\end{equation}
\end{definition}

\begin{definition}[Convolution Operator]  
An LTI system $H$ is a convolution operator if
\begin{equation}
H(u(t)) = (h * u)(t),
\end{equation}
where $h(\cdot)$ is called the \textbf{kernel} (impulse response) of $H$.  
\end{definition}

\begin{theorem}[Lp Stability]  
Consider an LTI system $H$ with impulse response
\begin{equation}
h(t) = h_0 \delta(t) + h_a(t).
\end{equation}
Then $H$ is $L_p$ stable if and only if $h \in A$, and moreover
\begin{equation}
\|Hu\|_{L_p} \;\leq\; \|h\|_{L_p}\, \|u\|_{L_p}.
\end{equation}
\end{theorem}

\section{\texorpdfstring{$L_p$ Gains}{Lp Gains}}\index{Lp Gains}

We now focus on the study of input-output gains for Linear Time-Invariant (LTI) systems.  
For simplicity, we restrict attention to single-input single-output (SISO) systems.  
The notion of gain depends essentially on the choice of input function space.

%------------------------------------------------
\subsection{$L_\infty$ Gain}\index{Lp Gains!$L_\infty$ Gain}

\begin{definition}[$L_\infty$ Gain]
For an LTI system $H$ with impulse response $h(t) = h_0 \delta(t) + h_a(t)$, the $L_\infty$ gain is defined as
\begin{equation}
\gamma_\infty(H) \;=\; \sup_{u \neq 0} \frac{\|Hu\|_\infty}{\|u\|_\infty}.
\end{equation}
\end{definition}

\begin{theorem}
The $L_\infty$ gain of $H$ is given by the $L_1$ norm of the impulse response:
\begin{equation}
\gamma_\infty(H) = \|h\|_1 = |h_0| + \int_0^\infty |h_a(\tau)| \, d\tau.
\end{equation}
\end{theorem}

%------------------------------------------------
\subsection{$L_2$ Gain}\index{Lp Gains!$L_2$ Gain}

\begin{definition}[$L_2$ Gain]
For an LTI system $H$ with impulse response $h \in L_2$, the $L_2$ gain is defined as
\begin{equation}
\gamma_2(H) \;=\; \sup_{u \neq 0} \frac{\|Hu\|_2}{\|u\|_2}.
\end{equation}
\end{definition}

\begin{theorem}
The $L_2$ gain of $H$ equals the $H_\infty$ norm of its transfer function:
\begin{equation}
\gamma_2(H) \;=\; \|H(j\omega)\|_\infty 
= \sup_{\omega \in \mathbb{R}} |H(j\omega)|.
\end{equation}
\end{theorem}

\section{Closed-Loop Input-Output Stability}\index{Closed-Loop Stability}

We now study stability of feedback interconnections using the input-output framework.  
Consider two subsystems $H_1, H_2 : X_e \to X_e$ interconnected as in Figure~\ref{fig:feedback}.

\begin{definition}[Feedback System]
The feedback system satisfies for inputs $u_1,u_2 \in X_e$:
\begin{align}
e_1 &= u_1 - H_2 e_2, \label{eq:fb1}\\
e_2 &= u_2 + H_1 e_1, \label{eq:fb2}
\end{align}
with outputs $y_1 = H_1 e_1$, $y_2 = H_2 e_2$.  
The signals $e_1,e_2,y_1,y_2$ are called the \emph{internal errors and outputs}.
\end{definition}

\begin{definition}[Closed-Loop Relations]
Define
\begin{equation}
E = \{ (u,e) \in X_e \times X_e \;\mid\; e \text{ satisfies } \eqref{eq:fb1}, \eqref{eq:fb2}\},
\end{equation}
\begin{equation}
F = \{ (u,y) \in X_e \times X_e \;\mid\; y \text{ satisfies } \eqref{eq:fb1}, \eqref{eq:fb2}\}.
\end{equation}
\end{definition}

\begin{definition}[Boundedness]
A relation $P$ on $X_e$ is said to be \emph{bounded} if it does not enlarge signals without limit.  
Formally, for every bounded set $B \subseteq \mathrm{dom}(P)$, the image
\[
P(B) = \{\, y \in X_e \mid \exists x \in B \ \text{such that}\ (x,y) \in P \,\}
\]
is also a bounded subset of $X_e$.

In simpler terms: if all possible inputs taken from $B$ remain bounded, then all the corresponding outputs produced by $P$ also remain bounded.
\end{definition}

\begin{definition}[Closed-Loop Stability]
The feedback interconnection is said to be \emph{input-output stable} (or simply stable) if the relations $E$ and $F$ are bounded.  
Equivalently, for all bounded inputs $u_1,u_2 \in X_e$, the signals $e_1,e_2,y_1,y_2$ also belong to $X_e$.
\end{definition}

\begin{remark}
Stability depends on the choice of function space $X_e$; we may write ``$X_e$-stable'' when emphasis is needed.
\end{remark}

\begin{figure}[h!]
\centering
\begin{tikzpicture}[auto, node distance=2cm, thick]

  % Nodes
  \node (u1) {$u_1(t)$};
  \node[circle, draw, minimum size=8mm, right=of u1] (sum1) {};
  \node[draw, rectangle, minimum height=1.2cm, minimum width=2cm, right=of sum1] (H1) {$H_1$};
  \node[right=of H1] (y1) {$y_1(t)$};
  
  \node[circle, draw, minimum size=8mm, below=2.5cm of y1] (sum2) {};
  \node[draw, rectangle, minimum height=1.2cm, minimum width=2cm, left=of sum2] (H2) {$H_2$};
  \node[right=of sum2] (u2) {$u_2(t)$};
  
  % Connections
  \draw[->] (u1) -- (sum1);
  \draw[->] (sum1) -- node {$e_1(t)$} (H1);
  \draw[->] (H1) -- (y1);
  \draw[->] (y1) -- (sum2);
  \draw[->] (sum2) -- node {$e_2(t)$} (H2);
  \draw[->] (H2) -| node[pos=0.9, left] {$y_2(t)$} (sum1);
  \draw[->] (u2) -- (sum2);

  % Signs inside sum circles
  \node at (sum1.center) {$+$};
  \node at ([xshift=-4pt,yshift=-6pt]sum1.center) {$-$};
  
  \node at (sum2.center) {$+$};
  \node at ([xshift=6pt,yshift=-4pt]sum2.center) {$+$};

\end{tikzpicture}
\caption{The feedback system $S$.}
\label{fig:feedback}
\end{figure}

\section{The Small Gain Theorem}\index{The Small Gain Theorem}

In this section we study the so-called \textbf{small gain theorem}, one of the most important results in the theory of input-output systems. The main goal of the theorem is to provide open-loop conditions for closed-loop stability. 

\begin{definition}[System Gain]
For a causal operator (system) $H: X_e \to X_e$, its \emph{gain} $\gamma(H)$ is defined as
\begin{equation}
\gamma(H) = \sup_{u \neq 0} \frac{\|Hu\|}{\|u\|}.
\end{equation}
This quantity measures the maximum amplification of the input signal energy (or norm) that the system can produce at the output. 
\end{definition}

\begin{itemize}
    \item If $\gamma(H)<\infty$, the system is said to have \emph{finite gain}.  
    \item For an LTI system with transfer function $G(s)$, the gain is given by the $\mathcal{H}_\infty$ norm:
    \begin{equation}
    \gamma(H) = \sup_{\omega \in \mathbb{R}} |G(j\omega)|.
    \end{equation}
    That is, the largest magnitude of the frequency response.  
    \item In simple terms, $\gamma(H)$ tells us the worst-case signal amplification. It is extremely useful in feedback analysis, because if the product of gains in a loop is less than one, signals are guaranteed not to blow up.
\end{itemize}

\begin{theorem}[Small Gain Theorem]
Consider the feedback interconnection of two systems as shown in Figure \ref{fig:feedback}
\begin{equation}
H_1, H_2 : X_e \to X_e.
\end{equation}
If 
\begin{equation}
\gamma(H_1)\,\gamma(H_2) < 1,
\end{equation}
then the feedback system is input-output stable.
\end{theorem}

\begin{remark}
 The theorem gives a \textbf{sufficient condition} for stability, not necessary. Some systems with $\gamma(H_1)\gamma(H_2)\geq 1$ may still be stable. Existence of solutions is not guaranteed by the theorem; it only ensures boundedness if solutions exist.
\end{remark}

\begin{example}[Nonlinear Feedback]
Let $H_1$ be an LTI system with transfer function
\begin{equation}
G(s) = \frac{-2}{s^2+2s+4},
\end{equation}
and $H_2$ a memoryless nonlinearity $N(\cdot)$ as in Fig.\ref{fig:nonlinear}.

The gain of $H_1$ is
\begin{equation}
\gamma(H_1) = \sup_{\omega} |G(j\omega)|.
\end{equation}
After computation, $\gamma(H_1) = \tfrac{1}{12}$.

If $N(\cdot)$ has slope $k$, then $\gamma(H_2) = |k|$.  
The small gain condition requires
\begin{equation}
|k| < 12.
\end{equation}

\centering
\begin{tikzpicture}[scale=1.2, thick]
  % Axes
  \draw[->] (-2.5,0) -- (2.5,0) node[right] {$x$};
  \draw[->] (0,-1.5) -- (0,1.5) node[above] {$N(x)$};

  % Saturation characteristic
  \draw (-2,-1) -- (-1,-1) -- (1,1) -- (2,1);

  % Slope indicator
  \draw (0.5,0.5) -- (0.7,0.5) -- (0.7,0.7);
  \node at (0.8,0.4) {$K$};
\end{tikzpicture}
\captionof{figure}{The nonlinearity $N(\cdot)$.}
\label{fig:nonlinear}
\end{example}

\begin{example}[Constant Gain Feedback]
Let $H_2 = k$ (a constant gain). Then $\gamma(H_2)=|k|$.

Small gain theorem yields stability if
\begin{equation}
|k| < 12.
\end{equation}

Pole analysis of
\begin{equation}
H(s) = \frac{G(s)}{1+kG(s)} = \frac{-2}{s^2+2s+4+k}
\end{equation}
gives stability iff $k>-4$.

\textbf{Conclusion:} small gain theorem gives $-12<k<12$, while exact pole test gives $k>-4$. Thus, the small gain theorem provides only a conservative (sufficient) stability condition.
\end{example}

\section{Loop Transformations}

As we have seen, the small gain theorem provides sufficient conditions for the stability of a feedback loop, and very often results in conservative estimates of the system stability.  
The same occurs with any other sufficient but not necessary stability condition, such as the passivity theorem (to be discussed in Chapter~8).  

One way to obtain improved stability conditions (i.e., less conservative) is to apply the theorems to a modified feedback loop that satisfies the following two properties:  

\begin{enumerate}
\item  it guarantees stability of the original feedback loop, and  
\item  it lessens the overall requirements over $H_1$ and $H_2$. 
\end{enumerate}

In other words, it is possible that a modified system satisfies the stability conditions imposed by the theorem in use whereas the original system does not.  
There are two basic transformations of feedback loops that will be used throughout the book, and will be referred to as \textbf{transformations of Type I and Type II}.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{Images/nonlinear/ipopstability/sk.png}
    \caption{The feedback system $S_k$}
    \label{fig:feedback_sk}
\end{figure}


\begin{definition}[Type I Loop Transformation]
Consider the feedback system $S$ of Figure~\ref{fig:feedback}.  
Let $H_1, H_2, K$ and $(I+KH_1)^{-1}$ be causal maps from $X_e$ into $X$, and assume that $K$ is linear.  

A loop transformation of Type I is defined to be the modified system, denoted $S_K$, formed by the feedback interconnection of the subsystems
\begin{equation}
H_1' = H_1 (I+KH_1)^{-1}, \qquad H_2' = H_2 - K,
\end{equation}
with inputs
\begin{equation}
u_1' = u_1 - Ku_2, \qquad u_2' = u_2,
\end{equation}
as shown in Figure~\ref{fig:feedback_sk}.  

The closed-loop relations of $S_K$ will be denoted $E_K$ and $F_K$.
\end{definition}

\begin{theorem}
Consider the system $S$ of Figure~\ref{fig:feedback} and let $S_K$ be the modified system obtained after a Type I loop transformation.  
Assume $K$ and $(I+KH_1)^{-1}: X \to X$. Then:
\begin{equation}
S \text{ is stable } \iff S_K \text{ is stable.}
\end{equation}
\end{theorem}

\begin{figure}[h]
\centering
\begin{tikzpicture}[auto, node distance=1.5cm, thick, >=latex]
  % Nodes
  \node (u1) {$u_1$};
  \node[draw, rectangle, minimum width=1.5cm, minimum height=1cm, right=of u1] (Minv) {$M^{-1}$};
  \node[circle, draw, minimum size=8mm, right=1.8cm of Minv] (sum1) {};
  \node[draw, rectangle, minimum width=1.5cm, minimum height=1cm, right=2.1cm of sum1] (M) {$M$};
  \node[draw, rectangle, minimum width=1.5cm, minimum height=1cm, right=of M] (H1) {$H_1$};

  \node[draw, rectangle, minimum width=1.5cm, minimum height=1cm, below=2.2cm of sum1] (Minv2) {$M^{-1}$};
  \node[draw, rectangle, minimum width=1.5cm, minimum height=1cm, right=of Minv2] (H2) {$H_2$};
  \node[circle, draw, minimum size=8mm, right=2.1cm of H2] (sum2) {};
  \node[right=2.1cm of sum2] (u2) {$u_2$};

  % Connections - Top path
  \draw[->] (u1) -- (Minv);
  \draw[->] (Minv) -- node[above] {$u_1'$} (sum1); 
  \draw[->] (sum1) -- node[above] {$x_1$} (M);
  \draw[->] (M) -- node[above] {$e_1$} (H1);

  % Feedback from H1 output to sum2 (no separate y1 node)
  \draw[->] (H1) |- ++(0,-2) -| (sum2);

  % Bottom path
  \draw[->] (Minv2) -- ++(0,2.2cm) -| node[pos=0.25,left] {$z_2$} (sum1);  
  \draw[->] (H2) -- node[below] {$y_2$} (Minv2);
  \draw[->] (sum2) -- node[below] {$e_2$} (H2);
  \draw[->] (u2) -- (sum2);

  % Labels on connections
  \node at ([xshift=1.3cm]sum1) [above] {};
  \node at ([xshift=1.15cm]M) [above] {};
  \node at ([xshift=0cm,yshift=-1.2cm]sum1) [left] {};
  \node at ([xshift=1.3cm]Minv2) [below] {};
  \node at ([xshift=1.3cm]H2) [below] {};

  % Signs
  \node at (sum1.center) {$+$};
  \node at ([xshift=-5pt, yshift=-7.5pt]sum1.center) {$-$};
  \node at (sum2.center) {$+$};
  \node at ([xshift=7.5pt, yshift=-5pt]sum2.center) {$+$};

\end{tikzpicture}
\caption{The Feedback System $S_M$}
\label{fig:feedback_sm}
\end{figure}


\begin{definition}[Type II Loop Transformation]
Consider the feedback system $S$ of Figure~\ref{fig:feedback}.  
Let $H_1, H_2$ be causal maps of $X_e$ into $X$, and let $M$ be a causal linear operator satisfying:  

\begin{enumerate}
    \item $M: X \to X$,  
    \item $M^{-1}: X \to X$ with $MM^{-1}=I$, and $M^{-1}$ is causal,  
    \item both $M$ and $M^{-1}$ have finite gain.  
\end{enumerate}

A Type II loop transformation is defined to be the modified system $S_M$, formed by the feedback interconnection of the subsystems
\begin{equation}
H_1' = H_1 M, \qquad H_2' = M^{-1} H_2,
\end{equation}
with inputs
\begin{equation}
u_1' = M^{-1}u_1, \qquad u_2' = u_2,
\end{equation}
as shown in Figure~\ref{fig:feedback_sm}.  

The closed-loop relations of this modified system will be denoted $E_M$ and $F_M$.
\end{definition}

\begin{theorem}
Consider the system $S$ of Figure~\ref{fig:feedback} and let $S_M$ be the modified system obtained after a Type II loop transformation. Then:
\begin{equation}
S \text{ is stable } \iff S_M \text{ is stable.}
\end{equation}
\end{theorem}

%------------------------------------------------
\section{The Circle Criterion}\index{Circle Criterion}

Historically, one of the first major applications of the small gain theorem was the celebrated \emph{circle criterion}, which provides frequency-domain conditions for the $\mathcal{L}_2$ stability of nonlinear feedback systems.  
The key idea is to combine the input-output viewpoint (sector nonlinearities) with the frequency-domain viewpoint (Nyquist plots).

\begin{definition}[Sector Condition]
A function $\phi:\mathbb{R}^+\times\mathbb{R}\to\mathbb{R}$ belongs to the sector $[\alpha,\beta]$ with $\alpha<\beta$ if
\begin{equation}
    \alpha x^2 \leq x\phi(t,x) \leq \beta x^2, 
    \qquad \forall t\geq 0,\ \forall x\in\mathbb{R}.
\end{equation}
Intuitively, this means that the slope of the nonlinearity $\phi$ always lies between $\alpha$ and $\beta$.
\end{definition}

Before introducing the circle criterion itself, we recall the Nyquist stability test for linear feedback systems.

\begin{definition}[Nyquist Plot]
The \emph{Nyquist plot} of a transfer function $G(s)$ is the locus of points
\[
G(j\omega), \qquad -\infty < \omega < \infty,
\]
in the complex plane.  
It represents how the system responds to sinusoidal inputs of frequency $\omega$, with the magnitude and phase encoded as the distance and angle of $G(j\omega)$ from the origin.
\end{definition}

\begin{proposition}[Nyquist Criterion]
Consider a feedback system where the open-loop transfer function is
\[
G(s) = g(s) + \frac{n(s)}{d(s)},
\]
with $g(s)$ stable, $n(s), d(s)$ polynomials, and all zeros of $d(s)$ in the closed right-half plane.  
Let $v$ denote the number of open right-half plane poles of $d(s)$.  

Then the closed-loop system with constant feedback gain $K$ is $\mathcal{L}_p$-stable ($1<p<\infty$) if and only if the Nyquist plot of $G(j\omega)$
\begin{itemize}
    \item does not pass through the critical point $\big(-\tfrac{1}{K},0\big)$, and  
    \item encircles $\big(-\tfrac{1}{K},0\big)$ exactly $v$ times in the counterclockwise direction as $\omega$ varies from $-\infty$ to $+\infty$.
\end{itemize}
\end{proposition}

\begin{theorem}[Circle Criterion]
Consider the feedback of an LTI system $G(s)$ and a static nonlinearity $\phi$ in the sector $[\alpha,\beta]$.  
The closed-loop is $\mathcal{L}_2$-stable if the Nyquist plot of $G(j\omega)$ avoids a certain \emph{critical circle} in the complex plane, defined by:
\begin{itemize}
    \item It passes through $(-1/\alpha,0)$ and $(-1/\beta,0)$ on the real axis.  
    \item Its center lies on the real axis.  
\end{itemize}
The exact condition depends on whether the sector includes the origin.

\begin{center}
\begin{tikzpicture}[scale=1.2]
    % Axes
    \draw[->] (-3.5,0) -- (1,0) node[right] {$\Re$};
    \draw[->] (0,-2) -- (0,2) node[above] {$\Im$};

    % Sector points
    \coordinate (A) at (-3,0); % -1/alpha
    \coordinate (B) at (-1,0); % -1/beta
    \coordinate (M) at (-2,0); % midpoint

    % Critical circle
    \draw[dashed] (M) circle (1);

    % Mark points
    \fill (A) circle (1.5pt) node[below] {$-\tfrac{1}{\alpha}$};
    \fill (B) circle (1.5pt) node[below] {$-\tfrac{1}{\beta}$};
    \fill (M) circle (1.5pt) node[below,yshift=-2pt] {center};

    % Example Nyquist plot (safe, staying outside the circle)
    \draw[thick,blue,->,>=stealth] 
        (-3.4,-0.5) .. controls (-3.2,2) and (-0.5,1.5) .. (-0.6,-0.8);

    % Labels
    \node at (-2,1.8) {Critical circle $C'$};
    \node[blue] at (-1,1.3) {Nyquist plot};
\end{tikzpicture}
\captionof{figure}{Circle Criterion: the Nyquist plot must avoid the critical circle $C'$.}
\end{center}

\end{theorem}



%------------------------------------------------

