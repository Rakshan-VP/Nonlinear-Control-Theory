\chapterimage{orange2.jpg} % Chapter heading image
\chapterspaceabove{6.75cm} % Whitespace from the top of the page to the chapter title on chapter pages
\chapterspacebelow{7.25cm} % Amount of vertical whitespace from the top margin to the start of the text on chapter pages

\chapter{Mathematical Preliminaries}\index{Mathematical Preliminaries}
\section{Overview}\index{Overview}
In this chapter, we introduce key mathematical tools that will be used throughout the book, such as matrices, vector spaces, norms, and inner products. These concepts provide the foundation for analyzing systems, describing their behavior, and formulating methods for control and stability. Our focus will be on essential definitions, properties, and operations, presented in a clear and application-oriented manner. The aim is to build a working knowledge of these preliminaries that can be readily applied in the upcoming chapters.


\section{Sets}\index{Sets}

\begin{definition}[Set]
A \emph{set} is a well-defined collection of distinct objects, considered as an object in its own right. The objects in a set are called its \emph{elements} or \emph{members}, and if $x$ is an element of a set $A$, we write $x \in A$. 
\end{definition}

\begin{notation}
Commonly used notations for sets include:
\begin{itemize}
    \item $\mathbb{N}$: Natural numbers $\{1,2,3,\dots\}$
    \item $\mathbb{Z}$: Integers $\{\dots,-2,-1,0,1,2,\dots\}$
    \item $\mathbb{Q}$: Rational numbers
    \item $\mathbb{R}$: Real numbers
    \item $\mathbb{C}$: Complex numbers
\end{itemize}
\end{notation}

\begin{example}[Set Operations]
Let $A = \{1,2,3,4\}$ and $B = \{3,4,5,6\}$.  
\begin{itemize}
    \item Union: $A \cup B = \{1,2,3,4,5,6\}$  
    \item Intersection: $A \cap B = \{3,4\}$  
    \item Difference: $A \setminus B = \{1,2\}$  
    \item Cartesian Product: $A \times B = \{(a,b)\,|\, a \in A, b \in B\}$  
\end{itemize}
\end{example}

\begin{remark}
Sets may be finite or infinite. Infinite sets such as $\mathbb{N}$ and $\mathbb{R}$ play a central role in mathematics, especially in analysis and control theory.
\end{remark}

%------------------------------------------------
\section{Metric Spaces}\index{Metric Spaces}

\begin{definition}[Metric Space]
A \emph{metric space} is a pair $(X,d)$ where $X$ is a non-empty set and 
$d : X \times X \to \mathbb{R}$ is a function, called a \emph{metric}, such that for all $x,y,z \in X$:
\begin{align}
    & d(x,y) \geq 0 \quad \text{(non-negativity)} \\
    & d(x,y) = 0 \iff x=y \quad \text{(identity of indiscernibles)} \\
    & d(x,y) = d(y,x) \quad \text{(symmetry)} \\
    & d(x,z) \leq d(x,y) + d(y,z) \quad \text{(triangle inequality)}
\end{align}
\end{definition}

\begin{notation}
Commonly used metrics include:
\begin{itemize}
    \item Euclidean metric: $d(x,y) = \|x-y\|_2$ on $\mathbb{R}^n$  
    \item Manhattan metric: $d(x,y) = \|x-y\|_1 = \sum |x_i - y_i|$  
    \item Maximum metric: $d(x,y) = \|x-y\|_\infty = \max |x_i - y_i|$  
\end{itemize}
\end{notation}

\begin{example}
Let $X = \mathbb{R}^2$ and define $d(x,y) = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$.  
Then $(\mathbb{R}^2,d)$ is a metric space with the standard Euclidean distance.
\end{example}

\begin{remark}
Every normed vector space $(V,\|\cdot\|)$ induces a metric by defining $d(x,y) = \|x-y\|$.  
Thus, metric spaces generalize the familiar notion of distance while allowing greater flexibility in analysis.
\end{remark}

%------------------------------------------------
\section{Vector Spaces}\index{Vector Spaces}

\begin{definition}[Vector Space]
A \emph{vector space} $V$ over a field $\mathbb{K}$ (where $\mathbb{K} = \mathbb{R}$ or $\mathbb{C}$) is a non-empty set together with two operations:
\begin{itemize}
    \item \textbf{Vector addition:} $+\ : V \times V \to V$
    \item \textbf{Scalar multiplication:} $\cdot\ : \mathbb{K} \times V \to V$
\end{itemize}
such that the usual axioms of associativity, commutativity, distributivity, and existence of additive identity and inverse hold.
\end{definition}

\begin{example}
The set $\mathbb{R}^n$ with componentwise addition and scalar multiplication is a vector space over $\mathbb{R}$.  
For instance, in $\mathbb{R}^2$:  
\begin{equation}
(x_1,x_2) + (y_1,y_2) = (x_1+y_1,\; x_2+y_2), \quad 
\lambda(x_1,x_2) = (\lambda x_1,\; \lambda x_2).
\end{equation}
\end{example}

%------------------------------------------------
\subsection{Linear Independence and Basis}\index{Vector Spaces!Linear Independence and Basis}

\begin{definition}[Linear Independence]
A set of vectors $\{v_1,v_2,\dots,v_k\}$ in $V$ is said to be \emph{linearly independent} if 
\begin{equation}
\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_k v_k = 0 \quad \Rightarrow \quad \alpha_1=\alpha_2=\cdots=\alpha_k=0.
\end{equation}
Otherwise, the set is \emph{linearly dependent}.
\end{definition}

\begin{definition}[Basis and Dimension]
A \emph{basis} of a vector space $V$ is a linearly independent set of vectors that spans $V$.  
The number of elements in a basis is called the \emph{dimension} of $V$, denoted $\dim(V)$.
\end{definition}

\begin{theorem}
In $\mathbb{R}^n$, any set of $n+1$ or more vectors is linearly dependent.
\end{theorem}

\begin{example}
In $\mathbb{R}^3$, the vectors $e_1=(1,0,0)$, $e_2=(0,1,0)$, and $e_3=(0,0,1)$ form a basis.  
They are linearly independent, and every vector $(x,y,z) \in \mathbb{R}^3$ can be written uniquely as 
\begin{equation}
(x,y,z) = x e_1 + y e_2 + z e_3.
\end{equation}
Hence, $\dim(\mathbb{R}^3)=3$.
\end{example}

%------------------------------------------------
\subsection{Subspaces}\index{Vector Spaces!Subspaces}

\begin{definition}[Subspace]
A subset $W \subseteq V$ is called a \emph{subspace} of $V$ if:
\begin{itemize}
    \item $0 \in W$,
    \item $u,v \in W \Rightarrow u+v \in W$,
    \item $u \in W, \lambda \in \mathbb{K} \Rightarrow \lambda u \in W$.
\end{itemize}
\end{definition}

\begin{definition}[Span]
Given a set of vectors $S = \{v_1,\dots,v_k\}$ in $V$, the \emph{span} of $S$, denoted $\mathrm{span}(S)$, is the set of all linear combinations of the vectors in $S$.  
That is, 
\begin{equation}
\mathrm{span}(S) = \Big\{ \sum_{i=1}^k \alpha_i v_i \; : \; \alpha_i \in \mathbb{K} \Big\}.
\end{equation}
\end{definition}

\begin{example}
In $\mathbb{R}^3$, let $S=\{(1,0,0), (0,1,0)\}$.  
The span of $S$ is 
\begin{equation}
\mathrm{span}(S) = \{(x,y,0) \in \mathbb{R}^3 \mid x,y \in \mathbb{R}\},
\end{equation}
which is a subspace of $\mathbb{R}^3$ corresponding to the $xy$-plane.
\end{example}

%------------------------------------------------
\subsection{Normed Vector Spaces}\index{Vector Spaces!Normed Vector Spaces}

\begin{definition}[Normed Vector Space]
A \emph{normed vector space} is a pair $(V,\|\cdot\|)$ where $V$ is a vector space and $\|\cdot\|: V \to \mathbb{R}$ satisfies, for all $x,y \in V$ and $\lambda \in \mathbb{K}$:
\begin{align}
& \|x\| \geq 0, \quad \|x\| = 0 \iff x=0 \quad \text{(positivity)} \\
& \|\lambda x\| = |\lambda| \|x\| \quad \text{(homogeneity)} \\
& \|x+y\| \leq \|x\| + \|y\| \quad \text{(triangle inequality)}
\end{align}
\end{definition}

\begin{theorem}[Hölder's Inequality]
For $x,y \in \mathbb{R}^n$ and $p,q > 1$ with $\tfrac{1}{p}+\tfrac{1}{q}=1$,  
\begin{equation}
\sum_{i=1}^n |x_i y_i| \;\leq\; \left( \sum_{i=1}^n |x_i|^p \right)^{1/p} 
\left( \sum_{i=1}^n |y_i|^q \right)^{1/q}.
\end{equation}
\end{theorem}

\begin{theorem}[Minkowski's Inequality]
For $x,y \in \mathbb{R}^n$ and $p \geq 1$,  
\begin{equation}
\left( \sum_{i=1}^n |x_i+y_i|^p \right)^{1/p} \;\leq\; 
\left( \sum_{i=1}^n |x_i|^p \right)^{1/p} + 
\left( \sum_{i=1}^n |y_i|^p \right)^{1/p}.
\end{equation}
\end{theorem}

\begin{example}
Consider $V=\mathbb{R}^2$ with the Euclidean norm $\|(x,y)\|_2=\sqrt{x^2+y^2}$.  
Then $(\mathbb{R}^2,\|\cdot\|_2)$ is a normed vector space.  
For example, $\|(3,4)\|_2=\sqrt{3^2+4^2}=5$.
\end{example}

%------------------------------------------------
\section{Matrices}\index{Matrices}

\subsection{Basic Properties}\index{Matrices!Basic Properties}

\begin{itemize}
    \item \textbf{Transpose:} For a matrix $A=(a_{ij}) \in \mathbb{R}^{m \times n}$, the transpose is $A^T=(a_{ji}) \in \mathbb{R}^{n \times m}$.  
    Properties:
    \begin{align*}
        (A^T)^T &= A, \\
        (A+B)^T &= A^T + B^T, \\
        (\alpha A)^T &= \alpha A^T, \quad \alpha \in \mathbb{R}, \\
        (AB)^T &= B^T A^T.
    \end{align*}
    
    \item \textbf{Symmetric Matrix:} $A \in \mathbb{R}^{n \times n}$ is symmetric if $A^T = A$.

    \item \textbf{Skew-Symmetric Matrix:} $A \in \mathbb{R}^{n \times n}$ is skew-symmetric if $A^T = -A$.

    \item \textbf{Orthogonal Matrix:} $A \in \mathbb{R}^{n \times n}$ is orthogonal if $A^T A = I$.  
    In this case, $A^{-1} = A^T$.

    \item \textbf{Inverse:} A square matrix $A$ is invertible (or nonsingular) if there exists $A^{-1}$ such that $AA^{-1}=A^{-1}A=I$.

    \item \textbf{Rank:} The rank of a matrix $A$, denoted $\mathrm{rank}(A)$, is the dimension of the column space (or row space) of $A$.
\end{itemize}

\begin{definition}[Null Space]
The \emph{null space} (or kernel) of $A \in \mathbb{R}^{m \times n}$ is
\[
\mathcal{N}(A) = \{x \in \mathbb{R}^n \mid Ax = 0\}.
\]
\end{definition}

\begin{theorem}[Rank-Nullity Theorem]
For $A \in \mathbb{R}^{m \times n}$,  
\[
\mathrm{rank}(A) + \dim(\mathcal{N}(A)) = n.
\]
\end{theorem}

%------------------------------------------------
\subsection{Eigenvalues, Eigenvectors and Diagonalization}\index{Matrices!Eigenvalues and Eigenvectors}

\begin{definition}[Eigenvalue and Eigenvector]
For a square matrix $A \in \mathbb{R}^{n \times n}$, a nonzero vector $v \in \mathbb{R}^n$ is an \emph{eigenvector} if 
\[
Av = \lambda v
\]
for some scalar $\lambda \in \mathbb{R}$, called an \emph{eigenvalue}.
\end{definition}

\begin{theorem}[Diagonalization]
If $A \in \mathbb{R}^{n \times n}$ has $n$ linearly independent eigenvectors, then $A$ is diagonalizable, i.e.,
\[
A = S D S^{-1},
\]
where $D$ is diagonal with the eigenvalues of $A$ on its diagonal, and $S$ is the matrix of eigenvectors.
\end{theorem}

\begin{theorem}[Special Case: Symmetric Matrices]
If $A \in \mathbb{R}^{n \times n}$ is real and symmetric, then:
\begin{itemize}
    \item All eigenvalues of $A$ are real.
    \item $A$ admits an orthogonal diagonalization: $A = Q D Q^T$, where $Q$ is orthogonal and $D$ diagonal.
\end{itemize}
\end{theorem}

%------------------------------------------------
\subsection{Quadratic Forms}\index{Matrices!Quadratic Forms}

A quadratic form on $\mathbb{R}^n$ associated with a matrix $A \in \mathbb{R}^{n \times n}$ is
\[
q(x) = x^T A x, \quad x \in \mathbb{R}^n.
\]
Without loss of generality, $A$ can be assumed symmetric.

\begin{definition}[Definiteness of Quadratic Forms]
Let $q(x) = x^T A x$, with $A$ symmetric.
\begin{itemize}
    \item $q$ is \textbf{positive definite} if $q(x) > 0$ for all $x \neq 0$.
    \item $q$ is \textbf{positive semidefinite} if $q(x) \geq 0$ for all $x$.
    \item $q$ is \textbf{negative definite} if $q(x) < 0$ for all $x \neq 0$.
    \item $q$ is \textbf{negative definite} if $q(x) \leq 0$ for all $x \neq 0$.
    \item $q$ is \textbf{indefinite} if it takes both positive and negative values.
\end{itemize}
\end{definition}

\begin{theorem}[Rayleigh’s Inequality]
For a symmetric matrix $A \in \mathbb{R}^{n \times n}$ with eigenvalues $\lambda_{\min} \leq \cdots \leq \lambda_{\max}$,  
the Rayleigh quotient
\[
R(x) = \frac{x^T A x}{x^T x}, \quad x \neq 0,
\]
satisfies
\[
\lambda_{\min} \leq R(x) \leq \lambda_{\max}.
\]
\end{theorem}

%------------------------------------------------
\section{Basic Topology}\index{Topology}

\subsection{Topology in Metric Spaces}\index{Topology!Metric Spaces}

Let $(X,d)$ be a metric space. We introduce the following concepts:

\begin{itemize}
    \item \textbf{Neighborhood:} A set $N_\epsilon(x) = \{y \in X : d(x,y) < \epsilon\}$ is called an $\epsilon$-neighborhood of $x \in X$.

    \item \textbf{Limit Point:} A point $x \in X$ is a limit point of a set $A \subseteq X$ if every neighborhood of $x$ contains a point of $A$ other than $x$ itself.

    \item \textbf{Interior Point:} A point $x \in A \subseteq X$ is an interior point of $A$ if there exists $\epsilon > 0$ such that $N_\epsilon(x) \subseteq A$.

    \item \textbf{Open Set:} A set $A \subseteq X$ is open if every point of $A$ is an interior point of $A$.

    \item \textbf{Complement:} The complement of $A \subseteq X$ is $A^c = X \setminus A$.

    \item \textbf{Closed Set:} A set $A \subseteq X$ is closed if $A^c$ is open, or equivalently, if $A$ contains all its limit points.

    \item \textbf{Bounded Set:} A set $A \subseteq X$ is bounded if there exists $x \in X$ and $M > 0$ such that $d(x,y) \leq M$ for all $y \in A$.
\end{itemize}

%------------------------------------------------
\subsection{Basic Topology in $\mathbb{R}^n$}\index{Topology!Euclidean Space}

Let $\mathbb{R}^n$ be equipped with the Euclidean metric $d(x,y) = \|x-y\|_2$.  

\begin{itemize}
    \item \textbf{Subspace of $\mathbb{R}^n$:} Any subset $A \subseteq \mathbb{R}^n$ considered with the induced metric is called a subspace.

    \item \textbf{Neighborhood:} For $x \in \mathbb{R}^n$ and $\epsilon > 0$, the $\epsilon$-neighborhood is the open ball $B_\epsilon(x) = \{y \in \mathbb{R}^n : \|x-y\| < \epsilon\}$.

    \item \textbf{Open Set:} A set $A \subseteq \mathbb{R}^n$ is open if for every $x \in A$, there exists $\epsilon > 0$ such that $B_\epsilon(x) \subseteq A$.

    \item \textbf{Bounded Set:} $A \subseteq \mathbb{R}^n$ is bounded if it lies inside some ball $B_R(0)$ of finite radius $R > 0$.

    \item \textbf{Compact Set:} $A \subseteq \mathbb{R}^n$ is compact if it is closed and bounded (Heine–Borel theorem).

    \item \textbf{Convex Set:} $A \subseteq \mathbb{R}^n$ is convex if for any $x,y \in A$ and $\lambda \in [0,1]$, we have $\lambda x + (1-\lambda)y \in A$.
\end{itemize}

\section{Sequences}\index{Sequences}

\begin{definition}[Sequence of Vectors in a Metric Space]
A \emph{sequence} in a metric space $(X,d)$ is a function $\{x_n\}_{n=1}^\infty$ from the natural numbers $\mathbb{N}$ into $X$, i.e., a list of elements $x_1, x_2, x_3, \dots$ of $X$.
\end{definition}

\begin{definition}[Limit of a Sequence]
A sequence $\{x_n\}$ in a metric space $(X,d)$ is said to \emph{converge} to $x \in X$ if for every $\epsilon > 0$, there exists $N \in \mathbb{N}$ such that
\[
d(x_n, x) < \epsilon \quad \forall n \geq N.
\]
We then write $\lim_{n \to \infty} x_n = x$.
\end{definition}

\begin{example}[Convergent Sequence in $\mathbb{R}$]
In $(\mathbb{R}, |\cdot|)$, the sequence $x_n = \frac{1}{n}$ converges to $0$ because for any $\epsilon > 0$, if $n > \frac{1}{\epsilon}$ then $|x_n - 0| = \frac{1}{n} < \epsilon$.
\end{example}

\begin{example}[Sequence in $\mathbb{R}^2$]
In $(\mathbb{R}^2, \|\cdot\|_2)$, the sequence $x_n = \left(\frac{1}{n}, \frac{1}{n^2}\right)$ converges to $(0,0)$ because $\|x_n - (0,0)\|_2 = \sqrt{\frac{1}{n^2} + \frac{1}{n^4}} \to 0$ as $n \to \infty$.
\end{example}

\begin{definition}[Cauchy Sequence]
A sequence $\{x_n\}$ in a metric space $(X,d)$ is called a \emph{Cauchy sequence} if for every $\epsilon > 0$, there exists $N \in \mathbb{N}$ such that
\[
d(x_m, x_n) < \epsilon \quad \forall m,n \geq N.
\]
\end{definition}

\begin{example}[Cauchy Sequence in $\mathbb{Q}$]
Consider the sequence defined by the decimal approximations of $\sqrt{2}$: $x_1 = 1$, $x_2 = 1.4$, $x_3 = 1.41$, $x_4 = 1.414$, etc.  
This is a Cauchy sequence in $\mathbb{Q}$ because the terms get arbitrarily close, but it does \emph{not} converge in $\mathbb{Q}$ (it converges to $\sqrt{2} \notin \mathbb{Q}$).
\end{example}

\begin{definition}[Complete Metric Space]
A metric space $(X,d)$ is \emph{complete} if every Cauchy sequence in $X$ converges to a point in $X$.
\end{definition}

\begin{example}[Complete Space]
The metric space $(\mathbb{R}, |\cdot|)$ is complete: every Cauchy sequence of real numbers converges to a real number.  
In contrast, $(\mathbb{Q}, |\cdot|)$ is not complete, as shown by the sequence of rational approximations to $\sqrt{2}$.
\end{example}

\section{Functions}\index{Functions}

\begin{definition}[Function]
Let $A$ and $B$ be two sets.  
A \emph{function} $f$ from $A$ to $B$, denoted $f:A \to B$, is a rule that assigns to each $a \in A$ a unique element $b \in B$, written $f(a) = b$.  
\index{Function}
\end{definition}

\begin{example}
Consider $f:\mathbb{R} \to \mathbb{R}$ defined by $f(x) = x^2$.  
For each $x \in \mathbb{R}$, the function assigns a unique value $x^2 \in \mathbb{R}$.  
\end{example}

\begin{definition}[Composition of Functions]
Let $f:A \to B$ and $g:B \to C$ be two functions.  
The \emph{composition} of $g$ with $f$ is the function $g \circ f : A \to C$ defined by
\[
(g \circ f)(x) = g(f(x)), \qquad \forall x \in A.
\]
\index{Function!Composition}
\end{definition}

\begin{example}
Let $f:\mathbb{R}\to \mathbb{R}$ be $f(x)=x^2$, and $g:\mathbb{R}\to \mathbb{R}$ be $g(x)=\sin(x)$.  
Then $(g \circ f)(x) = \sin(x^2)$.  
\end{example}

\begin{definition}[Continuity in Metric Spaces]
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces, and let $f:X \to Y$.  
The function $f$ is said to be \emph{continuous at} $x_0 \in X$ if for every $\epsilon > 0$, there exists $\delta > 0$ such that
\[
d_X(x,x_0) < \delta \;\;\Rightarrow\;\; d_Y(f(x), f(x_0)) < \epsilon.
\]
If $f$ is continuous at every point in $X$, then $f$ is called \emph{continuous on $X$}.  
\index{Continuity}
\end{definition}

\begin{example}
The function $f:\mathbb{R}\to \mathbb{R}$ defined by $f(x) = 3x+2$ is continuous everywhere since small changes in $x$ lead to proportionally small changes in $f(x)$.  
\end{example}

\begin{definition}[Uniform Continuity]
A function $f:X \to Y$ between metric spaces is called \emph{uniformly continuous} if for every $\epsilon > 0$, there exists $\delta > 0$ such that
\[
d_X(x_1,x_2) < \delta \;\;\Rightarrow\;\; d_Y(f(x_1), f(x_2)) < \epsilon
\]
for \emph{all} $x_1, x_2 \in X$.  
\index{Uniform Continuity}
\end{definition}

\begin{remark}
Continuity allows $\delta$ to depend on both $\epsilon$ and the point $x_0$.  
Uniform continuity requires a single $\delta$ (depending only on $\epsilon$) that works for the whole domain.  
\index{Continuity!Ordinary vs Uniform}
\end{remark}

\begin{example}
\begin{itemize}
    \item $f(x) = x^2$ is continuous on $\mathbb{R}$, but not uniformly continuous (as $x \to \infty$, the slope grows unbounded).  
    \item $f(x) = \sqrt{x}$ is uniformly continuous on $[0,\infty)$.  
\end{itemize}
\end{example}

\subsection{Bounded Linear Operators and Matrix Norms}\index{Functions!Bounded Linear Operators and Matrix Norms}

\begin{definition}[Linear Operator]
Let $X$ and $Y$ be vector spaces.  
A map $T:X \to Y$ is called a \emph{linear operator} if for all $x_1,x_2 \in X$ and scalars $\alpha,\beta \in \mathbb{R}$ (or $\mathbb{C}$),
\[
T(\alpha x_1 + \beta x_2) = \alpha T(x_1) + \beta T(x_2).
\]
\index{Operator!Linear}
\end{definition}

\begin{definition}[Bounded Linear Operator]
Let $(X,\|\cdot\|_X)$ and $(Y,\|\cdot\|_Y)$ be normed spaces.  
A linear operator $T:X \to Y$ is said to be \emph{bounded} if there exists $C>0$ such that
\[
\|Tx\|_Y \leq C \|x\|_X, \quad \forall x \in X.
\]
Equivalently, the \emph{operator norm} is defined as
\[
\|T\| = \sup_{x \neq 0} \frac{\|Tx\|_Y}{\|x\|_X} 
= \max_{\|x\|_X = 1} \|Tx\|_Y.
\]
\index{Operator!Bounded Linear}
\end{definition}

\begin{example}
Consider $T:\mathbb{R}^2 \to \mathbb{R}^2$ defined by $T(x,y) = (2x,3y)$.  
We compute:
\[
\|T(x,y)\|_\infty = \max(|2x|,|3y|).
\]
If $\|(x,y)\|_\infty = 1$, then the maximum value of $\|T(x,y)\|_\infty$ is $3$.  
Thus $\|T\|_\infty = 3$, and $T$ is bounded.  
\end{example}

\begin{definition}[Induced Matrix Norms]
Let $A \in \mathbb{R}^{n \times n}$.  
Given a vector norm $\|\cdot\|$ on $\mathbb{R}^n$, the \emph{induced matrix norm} is defined as
\[
\|A\| = \max_{\|x\|=1} \|Ax\|.
\]
\index{Matrix Norm}
\end{definition}

\begin{example}
For a matrix $A = (a_{ij}) \in \mathbb{R}^{n \times n}$, three common induced matrix norms are:
\begin{itemize}
    \item $\mathbf{1\text{-norm}}$:  
    \[
    \|A\|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^n |a_{ij}| 
    \quad \text{(maximum absolute column sum)}.
    \]
    \item $\mathbf{\infty\text{-norm}}$:  
    \[
    \|A\|_\infty = \max_{1 \leq i \leq n} \sum_{j=1}^n |a_{ij}| 
    \quad \text{(maximum absolute row sum)}.
    \]
    \item $\mathbf{2\text{-norm}}$:  
    \[
    \|A\|_2 = \sqrt{\lambda_{\max}(A^TA)}
    \quad \text{(spectral norm)}.
    \]
\end{itemize}
\end{example}

\section{Differentiability}\index{Differentiaility}

\begin{definition}[Differentiability in $\mathbb{R}$]
A function $f:\mathbb{R}\to \mathbb{R}$ is said to be \emph{differentiable} at $x_0\in \mathbb{R}$ if the limit
\[
f'(x_0) = \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)}{h}
\]
exists. The function $f'(x_0)$ is called the \emph{derivative} of $f$ at $x_0$.
\end{definition}

\begin{example}
The function $f(x)=x^2$ is differentiable everywhere with derivative $f'(x)=2x$.
\end{example}

\begin{example}
The function $f(x)=|x|$ is not differentiable at $x=0$, since the left and right limits of the difference quotient do not agree.
\end{example}


\begin{definition}[Differentiability in Higher Dimensions]
Let $f:\mathbb{R}^m\to \mathbb{R}^n$. We say $f$ is differentiable at $x_0\in \mathbb{R}^m$ if there exists a linear map 
$A:\mathbb{R}^m\to \mathbb{R}^n$ such that
\[
\lim_{h\to 0}\frac{\|f(x_0+h)-f(x_0)-A(h)\|}{\|h\|} = 0.
\]
The linear map $A$ is called the \emph{derivative} (or \emph{Jacobian matrix}) of $f$ at $x_0$.
\end{definition}

\begin{example}
Let $f:\mathbb{R}^2\to \mathbb{R}^2$ be given by $f(x,y)=(x^2+y,\, xy)$.  
Then 
\[
Df(x,y) = \begin{bmatrix}
2x & 1\\
y & x
\end{bmatrix}.
\]
\end{example}

\begin{definition}[Partial Derivative]
If $f:\mathbb{R}^m\to \mathbb{R}$, the \emph{partial derivative} with respect to $x_i$ at a point $x=(x_1,\dots,x_m)$ is
\[
\frac{\partial f}{\partial x_i}(x) = \lim_{h\to 0} \frac{f(x_1,\dots,x_i+h,\dots,x_m)-f(x)}{h}.
\]
\end{definition}

\begin{example}
For $f(x,y)=x^2y+e^y$, we have
\[
\frac{\partial f}{\partial x} = 2xy, \qquad \frac{\partial f}{\partial y} = x^2+e^y.
\]
\end{example}

\begin{definition}[Continuously Differentiable]
A function $f:\mathbb{R}^m\to \mathbb{R}^n$ is \emph{continuously differentiable} (denoted $C^1$) if it is differentiable and its derivative $Df(x)$ is continuous as a function of $x$.
\end{definition}

\begin{proposition}[Chain Rule]
Let $f:\mathbb{R}^m\to \mathbb{R}^n$ be differentiable at $x_0$, and $g:\mathbb{R}^n\to \mathbb{R}^p$ be differentiable at $f(x_0)$.  
Then $g\circ f:\mathbb{R}^m\to \mathbb{R}^p$ is differentiable at $x_0$, with
\[
D(g\circ f)(x_0) = Dg(f(x_0)) \cdot Df(x_0).
\]
\end{proposition}

\begin{example}
Let $f(x,y)=(x^2,y^2)$ and $g(u,v)=u+v$. Then $g\circ f(x,y)=x^2+y^2$.  
The chain rule gives
\[
D(g\circ f)(x,y)= [2x \quad 2y],
\]
which matches the direct derivative.
\end{example}

\begin{theorem}[Mean Value Theorem]
If $f:[a,b]\to \mathbb{R}$ is continuous on $[a,b]$ and differentiable on $(a,b)$, then there exists $c\in (a,b)$ such that
\[
f'(c) = \frac{f(b)-f(a)}{b-a}.
\]
\end{theorem}

\begin{example}
For $f(x)=x^2$ on $[1,3]$, the mean value theorem gives $c$ such that $2c=\frac{9-1}{2}=4$, hence $c=2$.
\end{example}

\begin{theorem}[Inverse Function Theorem]
Let $f:\mathbb{R}^n\to \mathbb{R}^n$ be a $C^1$ function.  
If $\det(Df(x_0))\neq 0$, then $f$ is locally invertible around $x_0$, and the inverse $f^{-1}$ is also $C^1$ near $f(x_0)$.  

Moreover, for $y=f(x)$ close to $f(x_0)$, the derivative of the inverse satisfies
\[
D(f^{-1})(y) = \big(Df(x)\big)^{-1}.
\]
\end{theorem}


\begin{example}
Consider $f:\mathbb{R}^2\to \mathbb{R}^2$, $f(x,y)=(e^x \cos y, e^x \sin y)$.  
At $(0,0)$, the Jacobian is 
\[
Df(0,0)=\begin{bmatrix}1&0\\0&1\end{bmatrix},
\]
which is invertible. Hence $f$ is locally invertible near $(0,0)$.
\end{example}

%------------------------------------------------
\section{Lipschitz Continuity}\index{Lipschitz Continuity}

\begin{definition}[Lipschitz Continuity]
    Let $f: \mathbb{R}^n \to \mathbb{R}^m$.  
    The function $f$ is said to be \emph{Lipschitz continuous} on a set $D \subseteq \mathbb{R}^n$ if there exists a constant $L \geq 0$ such that
    \begin{equation}
        \| f(x) - f(y) \| \leq L \| x - y \|, \qquad \forall x,y \in D.
    \end{equation}
    The smallest such constant $L$ is called the \emph{Lipschitz constant}.
\end{definition}

%------------------------------------------------

\begin{theorem}[Differentiable Functions are Locally Lipschitz]
    Let $f: \mathbb{R}^n \to \mathbb{R}^m$ be continuously differentiable ($C^1$).  
    Then $f$ is locally Lipschitz continuous, i.e., for every compact set $K \subset \mathbb{R}^n$, there exists a constant $L$ such that
    \[
        \| f(x) - f(y) \| \leq L \| x - y \|, \qquad \forall x,y \in K.
    \]
\end{theorem}

%------------------------------------------------

\begin{remark}
    If $f:\mathbb{R}^n \to \mathbb{R}^m$ is differentiable on $D$ and its Jacobian is bounded,
    \[
        M = \sup_{x \in D} \|Df(x)\| < \infty,
    \]
    then by the Mean Value Theorem,
    \[
        \|f(x)-f(y)\| \leq M \|x-y\|, \qquad \forall x,y \in D.
    \]
    Hence $f$ is Lipschitz on $D$ with constant $L=M$.  
    In simple terms: a bounded derivative ensures the function cannot change faster than linearly.
\end{remark}

%------------------------------------------------

\begin{example}
    Consider $f:\mathbb{R}\to \mathbb{R}$ given by $f(x) = 3x + 2$.  
    For all $x,y \in \mathbb{R}$,
    \[
        |f(x)-f(y)| = |3(x-y)| = 3|x-y|.
    \]
    Thus $f$ is Lipschitz continuous with Lipschitz constant $L=3$.
\end{example}

\begin{example}
    The function $f(x) = \sqrt{x}$ is Lipschitz continuous on $[0,1]$ but not on $[0,\infty)$, because the derivative $\frac{1}{2\sqrt{x}}$ becomes unbounded as $x \to 0^+$.
\end{example}

%------------------------------------------------
\section{Contraction Mapping}\index{Contraction Mapping}

\begin{definition}[Contraction Mapping]
    Let $(X,d)$ be a metric space.  
    A mapping $T: X \to X$ is called a \emph{contraction} if there exists a constant $0 \leq k < 1$ such that
    \[
        d(Tx,Ty) \leq k \, d(x,y), \qquad \forall x,y \in X.
    \]
    The constant $k$ is called the \emph{contraction constant}.
\end{definition}

%------------------------------------------------

\begin{theorem}[Contraction Mapping Principle / Banach Fixed Point Theorem]
    Let $(X,d)$ be a complete metric space and $T:X \to X$ a contraction mapping with contraction constant $0 \leq k < 1$.  
    Then:
    \begin{enumerate}
        \item There exists a unique fixed point $x^* \in X$ such that $T(x^*) = x^*$.
        \item For any initial point $x_0 \in X$, the sequence defined by iteration
        \[
            x_{n+1} = T(x_n)
        \]
        converges to $x^*$ as $n \to \infty$.
    \end{enumerate}
\end{theorem}

%------------------------------------------------
\section{Solution to Differential Equations}\index{Differential Equations}

\subsection{Linear Time-Invariant (LTI) Systems}\index{Differential Equations!LTI systems}

Consider the LTI system
\[
    \dot{x}(t) = A x(t) + B u(t), \qquad x(0) = x_0,
\]
where $A \in \mathbb{R}^{n \times n}$ and $B \in \mathbb{R}^{n \times m}$.

The solution is given by
\[
    x(t) = e^{At} x_0 + \int_0^t e^{A(t-\tau)} B u(\tau)\, d\tau.
\]

This closed-form expression follows from the matrix exponential and the variation of constants formula.

%------------------------------------------------

\subsection{Nonlinear Systems}\index{Differential Equations!Nonlinear Systems}

For nonlinear systems of the form
\[
    \dot{x}(t) = f(x(t),t), \qquad x(0) = x_0,
\]
an explicit solution formula generally does not exist, and one instead relies on qualitative theorems about existence and uniqueness of solutions.

%------------------------------------------------

\begin{theorem}[Local Existence and Uniqueness]
    Let $f:\mathbb{R}^n \times [0,T] \to \mathbb{R}^n$ be continuous in $t$ and locally Lipschitz in $x$.  
    Then, for every initial condition $x(0)=x_0$, there exists a time interval $[0,\delta]$, $\delta > 0$, on which the system has a unique solution $x(t)$.
\end{theorem}

%------------------------------------------------

\begin{theorem}[Global Existence and Uniqueness]
    If, in addition, $f(x,t)$ is globally Lipschitz in $x$ and continuous in $t$, then the solution $x(t)$ exists and is unique for all $t \geq 0$.
\end{theorem}
